<<<<<<< HEAD
# ğŸ§  AWS GenAI QA Projects

### ğŸ‘¨â€ğŸ’» Author: [Shailesh Gaikwad](https://www.linkedin.com/in/shaileshgaikwad9975/)
> QA Engineer (Manual Â· ADAS Â· AI/ML Testing)  
> Transitioning into **GenAI QA / Prompt Evaluation / AI Quality Engineering**

---

## ğŸ“˜ Overview
This repository contains a collection of **end-to-end AWS + GenAI QA projects**, designed to demonstrate real-world skills in **AI Testing**, **Prompt Evaluation**, and **Cloud QA Automation**.

All projects are tested locally using **Streamlit** and deployed with **AWS Bedrock**, **Lambda**, and **S3** services.

---

## ğŸ“‚ Project List

### ğŸ§© [Project 01 â€“ GenAI Chatbot (Claude 3.5 Sonnet)](project_01_chatbot_bedrock)
> Build and test a **Generative AI Chatbot** using **AWS Bedrock (Claude 3.5)** and **Streamlit UI**.  
> Includes QA evaluation to assess **clarity, accuracy, and relevance** of AI responses.

ğŸ”¹ Tech: `AWS Bedrock`, `Streamlit`, `boto3`, `Python`  
ğŸ”¹ Focus: *GenAI QA, Prompt Evaluation, Bedrock API Testing*

---

### ğŸ“„ [Project 02 â€“ AI Document Summarizer (Bedrock + Lambda + S3)](project_02_summarizer_lambda)
> Automatically generate **summaries** for uploaded documents stored in S3.  
> Uses AWS Lambda for automation and Claude for summarization.

ğŸ”¹ Tech: `AWS Lambda`, `S3`, `Bedrock`, `Python`  
ğŸ”¹ Focus: *Automation, File I/O Testing, Response Validation*

---

### ğŸ’¬ [Project 03 â€“ Prompt Evaluation Framework](project_03_prompt_eval_framework)
> Test multiple prompts against GenAI models and evaluate outputs.  
> Includes scoring metrics for **accuracy**, **completeness**, and **bias** detection.

ğŸ”¹ Tech: `Python`, `Bedrock`, `OpenAI Eval style`  
ğŸ”¹ Focus: *Prompt QA Evaluation, Test Automation, Scoring Systems*

---

### ğŸ” [Project 04 â€“ RAG QnA System (Retrieval-Augmented Generation)](project_04_rag_qna_system)
> Integrates AWS Bedrock with vector search for **document-based QnA**.  
> Tests how retrieval quality impacts GenAI answer accuracy.

ğŸ”¹ Tech: `Bedrock`, `LangChain`, `FAISS`, `Python`  
ğŸ”¹ Focus: *RAG Testing, Context Evaluation, Precision/Recall QA*

---

### âš–ï¸ [Project 05 â€“ Multi-Model Evaluation System](project_05_multi_model_eval)
> Compare responses from multiple GenAI models (Claude, Titan, Llama) side-by-side.  
> Evaluate performance across different QA metrics.

ğŸ”¹ Tech: `AWS Bedrock`, `Python`, `Streamlit`  
ğŸ”¹ Focus: *Model Comparison, Response Benchmarking, QA Metrics*

---

### ğŸ§  [Project 06 â€“ Human Feedback Evaluation System](project_06_human_feedback_system)
> Implements a **human-in-the-loop QA feedback system** for GenAI outputs.  
> Collects user feedback (ğŸ‘ğŸ‘) and updates model evaluation scores.

ğŸ”¹ Tech: `Streamlit`, `Bedrock`, `CSV Storage`, `Python`  
ğŸ”¹ Focus: *RLHF Simulation, Feedback QA, Continuous Improvement*

---

## ğŸ“ˆ Learning Goals
Each project builds progressively to strengthen your skills in:
- âœ… Manual & Exploratory Testing of GenAI Systems  
- âœ… Prompt Evaluation & Quality Metrics  
- âœ… AWS Bedrock Integration & Cloud QA  
- âœ… AI Output Validation (Accuracy, Clarity, Relevance)  
- âœ… Test Automation with Python  

---

## ğŸ§° Tech Stack Summary
| Category | Tools / Services |
|-----------|------------------|
| **Cloud / AI** | AWS Bedrock, Lambda, S3 |
| **Models** | Claude 3.5 Sonnet, Titan Text |
| **Frameworks** | Streamlit, LangChain |
| **Languages** | Python (boto3, json) |
| **QA Tools** | Manual QA, Prompt Testing, Evaluation Metrics |

---

## ğŸ“œ License
This repository is for **learning and portfolio demonstration purposes**.  
Feel free to fork or adapt for educational use with attribution.

---

## ğŸ Next Steps
Coming Soon:
- ğŸ§© Project 07 â€“ Automated Prompt Regression Testing  
- ğŸ“Š Project 08 â€“ GenAI Performance Benchmark Dashboard  

---

**â­ If you find this useful, give the repo a star and connect with me on LinkedIn!**  
> _â€œTesting AI is the art of teaching machines to think better.â€_

=======
ğŸ¤– Project 01 â€“ AWS GenAI Chatbot using Bedrock (Claude + Streamlit + Lambda)
ğŸ“š Overview

This project demonstrates how to build a Generative AI Chatbot powered by AWS Bedrock (Anthropic Claude model).
The chatbot allows users to interact with an AI model in real-time using a Streamlit frontend, while the AWS Lambda backend handles inference requests securely through Bedrock Runtime.

ğŸ§© Project Structure
project_01_chatbot_bedrock/
â”‚
â”œâ”€â”€ app.py                      # Streamlit UI for local chat interaction
â”œâ”€â”€ lambda_function.py          # AWS Lambda backend function
â”œâ”€â”€ test_bedrock_chatbot.py     # Local test script to validate Bedrock connection
â”œâ”€â”€ function.zip                # Deployment package for Lambda
â”œâ”€â”€ requirements.txt            # Project dependencies
â”œâ”€â”€ .gitignore                  # Ignore unnecessary files
â””â”€â”€ README.md                   # Project documentation

â˜ï¸ Architecture
User (Streamlit UI)
       â”‚
       â–¼
AWS Lambda Function  â”€â”€â–¶  AWS Bedrock Runtime  â”€â”€â–¶  Claude 3.5 Sonnet Model
       â”‚
       â–¼
  Returns AI response to Streamlit

âš™ï¸ Setup Instructions
1ï¸âƒ£ Prerequisites

Python 3.9+

AWS Account with Bedrock access

IAM role with bedrock:InvokeModel permission

Streamlit installed locally

2ï¸âƒ£ Local Environment Setup
# Clone the repo
git clone https://github.com/shailesh9975/aws-genai-qa-projects.git
cd aws-genai-qa-projects/project_01_chatbot_bedrock

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run Locally with Streamlit
streamlit run app.py


Then open in your browser:
ğŸŒ http://localhost:8501

Youâ€™ll see a chat interface where you can ask questions like:

â€œWhat is Generative AI QA testing?â€
â€œExplain AWS Bedrock in simple terms.â€

ğŸ§  Lambda Deployment
Step 1: Prepare Deployment Package
zip -r function.zip lambda_function.py

Step 2: Deploy on AWS Lambda

Go to AWS Console â†’ Lambda â†’ Create Function

Choose â€œAuthor from scratchâ€

Runtime: Python 3.10

Upload function.zip

Set environment variables (if needed)

Attach IAM role with bedrock:InvokeModel permission

ğŸ§ª Testing Locally
python test_bedrock_chatbot.py


Sample Output:

ğŸ¤– Claude Response:
Generative AI QA testing ensures quality and reliability of AI systems that create content...

ğŸ“Š Key Learnings

How to invoke AWS Bedrock models via boto3

Building a Streamlit-based GenAI frontend

Structuring cloud-based AI inference pipelines

Testing and validating Claude model outputs for QA

ğŸŒ Future Enhancements

âœ… Add multi-model support (Claude, Titan, Llama)
âœ… Integrate conversation history persistence (DynamoDB)
âœ… Deploy Streamlit app on AWS EC2 or Streamlit Cloud

ğŸ‘¨â€ğŸ’» Author

Shailesh Gaikwad
QA Engineer | AI/ML Validation | GenAI QA Tester
ğŸ”— GitHub | LinkedIn
>>>>>>> 93e07d9 (ğŸš€ Added AWS GenAI Chatbot Project (Claude + Streamlit + Lambda))
