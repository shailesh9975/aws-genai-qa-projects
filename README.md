# ğŸ§  AWS GenAI QA Projects

### ğŸ‘¨â€ğŸ’» Author: [Shailesh Gaikwad](https://www.linkedin.com/in/shaileshgaikwad9975/)
> QA Engineer (Manual Â· ADAS Â· AI/ML Testing)  
> Transitioning into **GenAI QA / Prompt Evaluation / AI Quality Engineering**

---

## ğŸ“˜ Overview
This repository contains a collection of **end-to-end AWS + GenAI QA projects**, each designed to demonstrate real-world skills in **AI Testing**, **Prompt Evaluation**, and **Cloud QA Automation**.  

All projects are tested locally using **Streamlit** and deployed with **AWS Bedrock**, **Lambda**, and **S3** services.

---

## ğŸ“‚ Project List

### ğŸ§© [Project 01 â€“ GenAI Chatbot (Claude 3.5 Sonnet)](project_01_chatbot_bedrock)
> Build and test a **Generative AI Chatbot** using **AWS Bedrock (Claude 3.5)** and **Streamlit UI**.  
> Includes QA evaluation layer to assess **clarity, accuracy, and relevance** of AI responses.

ğŸ”¹ Tech: `AWS Bedrock`, `Streamlit`, `boto3`, `Python`  
ğŸ”¹ Focus: *GenAI QA, Prompt Evaluation, Bedrock API Testing*

---

### ğŸ“„ [Project 02 â€“ AI Document Summarizer (Bedrock + Lambda + S3)](project_02_summarizer_lambda)
> Automatically generate **summaries** for uploaded documents stored in S3.  
> Uses AWS Lambda for automation and Claude for summarization.

ğŸ”¹ Tech: `AWS Lambda`, `S3`, `Bedrock`, `Python`  
ğŸ”¹ Focus: *Automation, File I/O Testing, Response Validation*

---

### ğŸ’¬ [Project 03 â€“ Prompt Evaluation Framework](project_03_prompt_eval_framework)
> A framework to test multiple prompts against GenAI models and evaluate outputs.  
> Includes scoring metrics for **accuracy**, **completeness**, and **bias** detection.

ğŸ”¹ Tech: `Python`, `Bedrock`, `OpenAI Eval style`  
ğŸ”¹ Focus: *Prompt QA Evaluation, Test Automation, Scoring Systems*

---

### ğŸ” [Project 04 â€“ RAG QnA System (Retrieval-Augmented Generation)](project_04_rag_qna_system)
> Integrates AWS Bedrock with vector search for **document-based QnA**.  
> Tests how retrieval quality impacts GenAI answer accuracy.

ğŸ”¹ Tech: `Bedrock`, `LangChain`, `FAISS`, `Python`  
ğŸ”¹ Focus: *RAG Testing, Context Evaluation, Precision/Recall QA*

---

### âš–ï¸ [Project 05 â€“ Multi-Model Evaluation System](project_05_multi_model_eval)
> Compare responses from multiple GenAI models (Claude, Titan, Llama) side-by-side.  
> Evaluate performance across different tasks and QA metrics.

ğŸ”¹ Tech: `AWS Bedrock`, `Python`, `Streamlit`  
ğŸ”¹ Focus: *Model Comparison, Response Benchmarking, QA Metrics*

---

### ğŸ§  [Project 06 â€“ Human Feedback Evaluation System](project_06_human_feedback_system)
> Implements a **human-in-the-loop QA feedback system** for GenAI outputs.  
> Collects user feedback (ğŸ‘ğŸ‘) and updates model evaluation scores.

ğŸ”¹ Tech: `Streamlit`, `Bedrock`, `CSV Storage`, `Python`  
ğŸ”¹ Focus: *RLHF Simulation, Feedback QA, Continuous Improvement*

---

## ğŸ“ˆ Learning Goals
Each project builds progressively to strengthen your skills in:
- âœ… Manual & Exploratory Testing of GenAI Systems  
- âœ… Prompt Evaluation & Quality Metrics  
- âœ… AWS Bedrock Integration & Cloud QA  
- âœ… AI Output Validation (Accuracy, Clarity, Relevance)  
- âœ… Test Automation with Python  

---

## ğŸ§° Tech Stack Summary
| Category | Tools / Services |
|-----------|------------------|
| **Cloud / AI** | AWS Bedrock, Lambda, S3 |
| **Models** | Claude 3.5 Sonnet, Titan Text |
| **Frameworks** | Streamlit, LangChain |
| **Languages** | Python (boto3, json) |
| **QA Tools** | Manual QA, Prompt Testing, Evaluation Metrics |

---

## ğŸ“œ License
This repository is for **learning and portfolio demonstration purposes**.  
Feel free to fork or adapt for educational use with attribution.

---

## ğŸ Next Steps
Coming Soon:
- ğŸ§© Project 07 â€“ Automated Prompt Regression Testing  
- ğŸ“Š Project 08 â€“ GenAI Performance Benchmark Dashboard

---

**â­ If you find this useful, give the repo a star and connect with me on LinkedIn!**
> _â€œTesting AI is the art of teaching machines to think better.â€_

