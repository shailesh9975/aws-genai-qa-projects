# üåç AWS + GenAI QA Project Portfolio

Welcome to the **AWS + GenAI QA Projects Repository** ‚Äî a hands-on collection of **AI Quality Assurance**, **Prompt Evaluation**, and **Automation Testing** projects built using **Amazon Bedrock**, **AWS Lambda**, and **Generative AI frameworks**.

This repository is designed to help both learners and professionals:
- Understand how **GenAI applications** are tested, validated, and benchmarked
- Gain **practical QA experience** using AWS services and open-source tools
- Explore **prompt evaluation**, **RAG systems**, and **human feedback loops**

---

## üß≠ Repository Structure

| Folder | Description |
|--------|--------------|
| [`project_01_chatbot_bedrock`](./project_01_chatbot_bedrock) | Chatbot powered by AWS Bedrock (Claude/Titan) |
| [`project_02_summarizer_lambda`](./project_02_summarizer_lambda) | Text summarization using AWS Lambda + Bedrock |
| [`project_03_prompt_eval_framework`](./project_03_prompt_eval_framework) | Framework for evaluating and scoring GenAI prompts |
| [`project_04_rag_qna_system`](./project_04_rag_qna_system) | Retrieval-Augmented Generation (RAG) based Q&A system |
| [`project_05_multi_model_eval`](./project_05_multi_model_eval) | Multi-model evaluation for comparing LLMs |
| [`project_06_human_feedback_system`](./project_06_human_feedback_system) | Human feedback system for refining model responses |
| [`docs`](./docs) | Documentation, test plans, and research notes |
| [`results`](./results) | Evaluation results, reports, and dashboards |
| [`prompts`](./prompts) | Prompt sets used across various experiments |

---

## ‚öôÔ∏è Tech Stack

- **AWS Services** ‚Äì Bedrock, Lambda, S3, API Gateway, OpenSearch  
- **Languages & Tools** ‚Äì Python, Streamlit, Jupyter, Postman, Git, AWS CLI  
- **QA Focus Areas** ‚Äì Functional testing, GenAI prompt evaluation, output validation, human-in-the-loop feedback  
- **AI Models** ‚Äì Claude, Titan, and other LLMs available via Bedrock

---

## üéØ Learning Objectives

Each project focuses on one or more of the following QA objectives:

- ‚úÖ Testing GenAI model outputs for **accuracy, bias, and hallucinations**  
- ‚úÖ Building frameworks for **automated evaluation and human feedback**  
- ‚úÖ Deploying AI systems with **reproducible test pipelines**  
- ‚úÖ Documenting test strategies and metrics for transparent evaluation  

---

## üöÄ How to Get Started

1. **Clone the repo**
   ```bash
   git clone https://github.com/yourusername/aws-genai-qa-projects.git
   cd aws-genai-qa-projects

